
############################################################################################################
##########################            RL2023 Assignment Answer Sheet              ##########################
############################################################################################################

# **PROVIDE YOUR ANSWERS TO THE ASSIGNMENT QUESTIONS IN THE FUNCTIONS BELOW.**

############################################################################################################
# Question 2
############################################################################################################

def question2_1() -> str:
    """
    (Multiple choice question):
    For the Q-learning algorithm, which value of gamma leads to the best average evaluation return?
    a) 0.99
    b) 0.8
    return: (str): your answer as a string. accepted strings: "a" or "b"
    """
    answer = "a"  # TYPE YOUR ANSWER HERE "a" or "b"
    return answer


def question2_2() -> str:
    """
    (Multiple choice question):
    For the First-visit Monte Carlo algorithm, which value of gamma leads to the best average evaluation return?
    a) 0.99
    b) 0.8
    return: (str): your answer as a string. accepted strings: "a" or "b"
    """
    answer = "a"  # TYPE YOUR ANSWER HERE "a" or "b"
    return answer


def question2_3() -> str:
    """
    (Multiple choice question):
    Between the two algorithms (Q-Learning and First-Visit MC), whose average evaluation return is impacted by gamma in
    a greater way?
    a) Q-Learning
    b) First-Visit Monte Carlo
    return: (str): your answer as a string. accepted strings: "a" or "b"
    """
    answer = "a"  # TYPE YOUR ANSWER HERE "a" or "b"
    return answer


def question2_4() -> str:
    """
    (Short answer question):
    Provide a short explanation (<100 words) as to why the value of gamma affects more the evaluation returns achieved
    by [Q-learning / First-Visit Monte Carlo] when compared to the other algorithm.
    return: answer (str): your answer as a string (100 words max)
    """
    answer = (
        "As Q-learning is an off-policy algorithm, it learns the optimal policy while using data generated by the behaviour policy which makes it more sensitive to changes in the value of gamma as compared to First-Visit Monte Carlo which is an on-policy algorithm.\n"
        "Secondly, unlike Monte Carlo, Q-learning allows bootstrapping where the algorithm involves the maximum future reward being weighted by gamma. This makes Q-learning more susceptible to changes in gamma.\n"
        "Finally, as Q-learning involves TD learning, the updates include estimations for the next state-action pair along with current rewards - this means that gamma directly impacts the evaluation returns as compared to First-Visit Monte Carlo."
    )
    return answer


############################################################################################################
# Question 3
############################################################################################################

def question3_1() -> str:
    """
    (Multiple choice question):
    In Reinforce, which learning rate achieves the highest mean returns at the end of training?
    a) 2e-2
    b) 2e-3
    c) 2e-4
    return: (str): your answer as a string. accepted strings: "a", "b" or "c"
    """
    answer = "b"  # TYPE YOUR ANSWER HERE "a", "b" or "c"
    return answer


def question3_2() -> str:
    """
    (Multiple choice question):
    When training DQN using a linear decay strategy for epsilon, which exploration fraction achieves the highest mean
    returns at the end of training?
    a) 0.99
    b) 0.75
    c) 0.01
    return: (str): your answer as a string. accepted strings: "a", "b" or "c"
    """
    answer = "c"  # TYPE YOUR ANSWER HERE "a", "b" or "c"
    return answer


def question3_3() -> str:
    """
    (Multiple choice question):
    When training DQN using an exponential decay strategy for epsilon, which epsilon decay achieves the highest
    mean returns at the end of training?
    a) 1.0
    b) 0.5
    c) 1e-5
    return: (str): your answer as a string. accepted strings: "a", "b" or "c"
    """
    answer = "c"  # TYPE YOUR ANSWER HERE "a", "b" or "c"
    return answer


def question3_4() -> str:
    """
    (Multiple choice question):
    What would the value of epsilon be at the end of training when employing an exponential decay strategy
    with epsilon decay set to 1.0?
    a) 0.0
    b) 1.0
    c) epsilon_min
    d) approximately 0.0057
    e) it depends on the number of training timesteps
    return: (str): your answer as a string. accepted strings: "a", "b", "c", "d" or "e"
    """
    answer = "b"  # TYPE YOUR ANSWER HERE "a", "b", "c", "d" or "e"
    return answer


def question3_5() -> str:
    """
    (Multiple choice question):
    What would the value of epsilon be at the end of  training when employing an exponential decay strategy
    with epsilon decay set to 0.95?
    a) 0.95
    b) 1.0
    c) epsilon_min
    d) approximately 0.0014
    e) it depends on the number of training timesteps
    return: (str): your answer as a string. accepted strings: "a", "b", "c", "d" or "e"
    """
    answer = "c"  # TYPE YOUR ANSWER HERE "a", "b", "c", "d" or "e"
    return answer


def question3_6() -> str:
    """
    (Short answer question):
    Based on your answer to question3_5(), briefly  explain why a decay strategy based on an exploration fraction
    parameter (such as in the linear decay strategy you implemented) may be more generally applicable across
    different environments  than a decay strategy based on a decay rate parameter (such as in the exponential decay
    strategy you implemented).
    return: answer (str): your answer as a string (100 words max)
    """
    answer = (
        "As exploration fraction depicts the percentage of total training steps completed, it is more adaptable and can be adjusted more easily to strike the preferred balance between exploration and exploitation while being environment-independent as compared to epsilon decay.\n"
        "Epsilon decay is more volatile than exploration fraction which makes it less consistent across different environments and provides less robust performance in complex environments as compared to exploration fraction.\n"
        "Moreover, exploration fraction is more intuitive to understand which makes it easier to fine tune for different environments which is not the case with the epsilon decay strategy."
    ) # TYPE YOUR ANSWER HERE (100 words max)
    return answer


def question3_7() -> str:
    """
    (Short answer question):
    In DQN, explain why the loss is not behaving as in typical supervised learning approaches
    (where we usually see a fairly steady decrease of the loss throughout training)
    return: answer (str): your answer as a string (150 words max)
    """
    answer = (
        "Firstly, as we have employed an epsilon-greedy action selection strategy, the agent may encounter unexpected outcomes or take suboptimal actions during exploration, resulting in unusual behaviour of the loss as the agent updates its q-values.\n"
        "Secondly, in RL, the target q-values are iteratively updated as the agent is being trained, however, in the case of supervised learning the loss is calculated using the training data. This leads to a non-monotonically decreasing DQN loss in RL rather than a consistent decrease that is observed in supervised learning.\n"
        "Moreover, DQN uses temporal difference learning to update the q-values based on the current rewards and expected future rewards. As the policy is unstable in the initial phases of training, we observe fluctuations in the DQN loss due to the iterative TD process.\n"
        "Finally, sampling randomly from the replay buffer introduces noise into training which leads to non-smooth changes in the loss."
    )  # TYPE YOUR ANSWER HERE (150 words max)
    return answer


def question3_8() -> str:
    """
    (Short answer question):
    Provide an explanation for the spikes which can be observed at regular intervals throughout
    the DQN training process.
    return: answer (str): your answer as a string (100 words max)
    """
    answer = (
        "There's a few reasons for the observed spikes:\n"
        "1. The target network is updated every 2000 steps which results in sudden changes in the target values at these time steps. While employing this technique helps with stabilising training as action selection and target value estimation are separated, it leads to spikes in the DQN loss.\n"
        "2. The random transitions we sample from the replay buffer may not be consecutive which leads to the loss varying from one batch to another, hence leading to the observed spikes.\n"
        "3. The epsilon-greedy action selection leads to unexpected outcomes or rewards during exploration which may also cause spikes in the loss."
    )  # TYPE YOUR ANSWER HERE (100 words max)
    return answer


############################################################################################################
# Question 5
############################################################################################################

def question5_1() -> str:
    """
    (Short answer question):
    Provide a short description (200 words max) describing your hyperparameter turning and scheduling process to get
    the best performance of your agents
    return: answer (str): your answer as a string (200 words max)
    """
    answer = "hi"  # TYPE YOUR ANSWER HERE (200 words max)
    return answer

